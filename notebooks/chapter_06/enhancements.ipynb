{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a59fc7",
   "metadata": {},
   "source": [
    "# Enhancements to RAG Pipelines (Optional)\n",
    "\n",
    "While a basic RAG pipeline retrieves and feeds relevant documents to a language model, several enhancements can improve accuracy, reliability, and efficiency:\n",
    "\n",
    "1. **Reranking**  \n",
    "   - After retrieving candidate documents, a reranker (often a cross-encoder model) can reorder results based on semantic relevance.  \n",
    "   - This ensures the most useful passages are prioritized before being passed to the language model.\n",
    "\n",
    "2. **Prompt Engineering**  \n",
    "   - Carefully designing prompts can guide the model to generate more accurate and structured answers.  \n",
    "   - Examples include: asking for step-by-step reasoning, enforcing answer formats, or limiting the scope of responses.\n",
    "\n",
    "3. **Handling Longer Context**  \n",
    "   - Many queries require large amounts of background knowledge. Techniques such as chunking, summarization, and hierarchical retrieval allow the model to handle long documents.  \n",
    "   - Recent models also support extended context windows, making it easier to process more text without losing coherence.\n",
    "\n",
    "These enhancements make RAG systems more robust and adaptable to real-world Q&A scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reranking example\n",
    "\n",
    "# imports and setup\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import warnings\n",
    "from langchain.schema import Document\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(\"..\", \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "MODEL_NAME = \"intfloat/e5-base-v2\"\n",
    "vec_store_path = os.path.join(project_root, \"data\", \"vector_store\", \"faiss_index\")\n",
    "# load the embedding model  \n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=MODEL_NAME, encode_kwargs={\"normalize_embeddings\": True})\n",
    "\n",
    "vectorstore = FAISS.load_local(vec_store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# dense retrieval\n",
    "def dense_search(query, k=10):\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    return results\n",
    "\n",
    "# Example search\n",
    "query = \"Give me cake recipe with chocolate and frosting\"\n",
    "results_dense = dense_search(query)\n",
    "for doc, score in results_dense:\n",
    "    print(f\"Score: {score:.4f}\\nContent: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "docs_only = [doc for doc, _ in results_dense]\n",
    "pairs = [(query, doc.page_content) for doc in docs_only]\n",
    "scores = reranker.predict(pairs)\n",
    "ranked = sorted(zip(docs_only, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nReranked Results:\\n\")\n",
    "for doc, score in ranked:\n",
    "    print(f\"Score: {score:.4f}\\nContent: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1768c6",
   "metadata": {},
   "source": [
    "The ms-marco-MiniLM-L-6-v2 model is a cross-encoder specialized for ranking passages in search tasks. Instead of just embedding text, it directly compares a query with a passage and produces a relevance score. This makes it particularly effective for sifting through large sets of documents to highlight the passages most likely to answer a question. Built on the compact and efficient MiniLM framework and fine-tuned on the MS MARCO dataset, the model achieves a strong trade-off between speed and accuracy, making it well-suited for real-world information retrieval systems.\n",
    "\n",
    "#### Note:\n",
    "For your use case, you might want to include **reranking**. Even if the initial retrieval step brings back relevant documents, they may not always be in the best order. A reranker model (like a cross-encoder) can take the query and each retrieved passage together, score their relevance more precisely, and reorder the results. This helps ensure that the most useful passages are placed at the top before being passed to the language model for answering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59b11c",
   "metadata": {},
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "1. **Ground the Response in Context**  \n",
    "   - Clearly instruct the model to use only the retrieved documents when answering.  \n",
    "   - Example: *\"Answer the question using only the context below. If the answer is not found, say 'I don’t know'.\"*\n",
    "\n",
    "2. **Be Explicit About the Task**  \n",
    "   - State exactly what you expect: summary, step-by-step reasoning, bullet points, or a direct answer.  \n",
    "   - Avoid vague instructions like *\"Tell me about this.\"*\n",
    "\n",
    "3. **Limit the Scope**  \n",
    "   - Prevent hallucinations by constraining the model’s role.  \n",
    "   - Example: *\"You are a research assistant. Only provide answers supported by the retrieved context.\"*\n",
    "\n",
    "4. **Handle Long or Multiple Contexts**  \n",
    "   - Use structured prompts (e.g., separating chunks with delimiters like `---`).  \n",
    "   - This helps the model distinguish between different sources.\n",
    "\n",
    "5. **Ask for Citations or Evidence**  \n",
    "   - Encourage the model to point back to the passages it used.  \n",
    "   - Example: *\"After your answer, cite the context snippet you used.\"*\n",
    "\n",
    "6. **Iterate and Test**  \n",
    "   - Small changes in wording can lead to big differences in output.  \n",
    "   - Always experiment with variations of the prompt to find what works best for your use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b89e582",
   "metadata": {},
   "source": [
    "# Handling Larger Context\n",
    "\n",
    "With many modern models offering generous context window sizes, you may not immediately face issues when working with short queries or smaller knowledge bases. However, challenges arise in **chat-based scenarios**, where the system needs to remember and build on past interactions. Over time, the accumulated history can exceed the model’s context limit.  \n",
    "\n",
    "To manage this, techniques such as **summarizing earlier turns**, **retrieving only the most relevant parts of the conversation**, or **using memory modules** can help. These approaches ensure the model stays coherent and context-aware without being overloaded by the full chat history.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
