{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3790ac7f",
   "metadata": {},
   "source": [
    "### Semantic Chunking  \n",
    "\n",
    "Imagine you’re reading a dessert recipe. Fixed-size chunking would chop the text into equal blocks, like cutting a cake with a ruler — sometimes you get the whole \"Add sugar and butter…\" step in one chunk, and sometimes you just get \"… for 10 minutes\" dangling in the next. Not very helpful.  \n",
    "\n",
    "Semantic chunking is more like cutting a layered cake along the frosting lines — you keep the steps and ideas intact. That way, \"Mix flour, sugar, and eggs\" stays together as one meaningful unit, instead of being split in the middle.  \n",
    "\n",
    "In short:  \n",
    "- **Fixed-size chunks**: Equal slices, context may get lost.  \n",
    "- **Semantic chunks**: Natural breaks, meaning preserved.  \n",
    "\n",
    "When working with recipes (or any domain text), semantic chunking ensures each instruction or paragraph stands on its own, making it easier for retrieval and for the model to understand.  \n",
    "\n",
    "For this exercise, I have created a separate chunk for each recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c4cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "project_root = os.path.abspath(os.path.join(\"..\", \"..\"))\n",
    "sys.path.append(project_root)\n",
    "from common.helper import read_pdf, extract_recipes_from_pdf\n",
    "\n",
    "pdf_path = os.path.join(project_root, \"data\", \"input\", \"recipe-book.pdf\")\n",
    "out_dir = os.path.join(project_root, \"data\", \"chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_recipes_from_pdf() reads the pdf spans and group the spans into logical sections like - directions and ingredients\n",
    "doc = read_pdf(pdf_path)\n",
    "\n",
    "skip_pages = 10   # skip initial 10 pages which do not have recipes\n",
    "end_page = 62    # end at page 62 which is the last page with recipes\n",
    "\n",
    "recipe_chunks = extract_recipes_from_pdf(doc, skip_pages=skip_pages, end_page=end_page)\n",
    "with open(f\"{out_dir}/recipe_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(recipe_chunks, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3f355",
   "metadata": {},
   "source": [
    "### Challenges in Extracting Recipes from PDFs\n",
    "\n",
    "While working with the recipe PDF, I faced a few key issues:\n",
    "\n",
    "- **Column layout**: Many pages are formatted with two columns. When reading the file directly with `fitz`, the extracted text often comes out of order, mixing left and right column content.  \n",
    "- **Non-recipe pages**: A significant number of pages don’t contain recipes at all (e.g., indexes, section dividers, or filler text), which adds noise if processed blindly.  \n",
    "- **Inconsistent formatting**: Even on recipe pages, headings, ingredients, and steps are not consistently separated. This makes it difficult to chunk recipes cleanly.  \n",
    "\n",
    "To address this, I wrote a small function that uses span-level information (e.g., font size, style, and color) to logically group text into meaningful sections before chunking. Also added page numbers in metadata.\n",
    "\n",
    "Not all PDFs are structured the same way, so handling them often requires custom parsing strategies. \n",
    "\n",
    "With these preprocessing steps in place, we are almost ready for **vectorization**.\n",
    "\n",
    "![semantic chunk](../../data/images/semantic_chunk.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab3bba",
   "metadata": {},
   "source": [
    "### (Optional) Handling Tables in PDFs  \n",
    "\n",
    "There are some pages at the end of the pdf for measurment guide. This information is structured as tables, we can also use this information to enrich our chunks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
