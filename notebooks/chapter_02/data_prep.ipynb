{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23433adc",
   "metadata": {},
   "source": [
    "### Chunking Exercise: The Baker’s Choice Challenge\n",
    "\n",
    "Imagine you’ve got your hands on a **Baker’s Choice dessert recipe book** — a glorious, chaotic collection of cakes, cookies, pies, mousses, and pastries, each more tempting (and complicated) than the last. \n",
    "\n",
    "Your mission? **Find the dessert recipe you actually want to make** without getting lost in the sugar-coated chaos.  \n",
    "\n",
    "---\n",
    "\n",
    "Sounds simple? Think again. This book also has experimental “fusion desserts,” random chef notes in the margins, and a few mysterious recipes written in code-like shorthand. If you feed the **entire book** to an LLM, it might start explaining how to make macarons when all you wanted was a brownie. \n",
    "\n",
    "---\n",
    "\n",
    "Enter **chunking** — your AI sous-chef. Instead of dumping the whole book on the model, you **slice it into bite-sized, meaningful chunks**. Now, the AI can find the exact dessert recipe without getting distracted by pastries it doesn’t need.  \n",
    "\n",
    "- **Goal**: Serve the model only the **chunks it can actually digest**, so it returns the right recipe, not a kitchen disaster.  \n",
    "- **Takeaway**: Chunking = making sure your AI finds the dessert, not a side of confusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73805438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "project_root = os.path.abspath(os.path.join(\"..\", \"..\"))\n",
    "sys.path.append(project_root)\n",
    "from common.helper import read_pdf, chunk_text, clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e215fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts text from a PDF, splits it into fixed-size chunks, and saves the chunks to a JSON file.\n",
    "# 1. Set chunk size and initialize variables.\n",
    "# 2. Read the PDF and concatenate text from all pages.\n",
    "# 3. Print a preview of the extracted text.\n",
    "# 4. Split the text into chunks and print chunk info.\n",
    "# 5. Save the chunks to a JSON file.\n",
    "\n",
    "chunk_size = 1000\n",
    "full_text = \"\"\n",
    "\n",
    "pdf_path = os.path.join(project_root, \"data\", \"input\", \"recipe-book.pdf\")\n",
    "doc = read_pdf(pdf_path)\n",
    "output_chunks_dir = os.path.join(project_root, \"data\", \"chunks\")\n",
    "\n",
    "\n",
    "for page in doc:\n",
    "        text = page.get_text()\n",
    "        if text:\n",
    "            full_text += text + \"\\n\"\n",
    "doc.close()\n",
    "\n",
    "#print(full_text[:500])  # Print the first 500 characters of the extracted text\n",
    "\n",
    "chunks = chunk_text(full_text, chunk_size=chunk_size)\n",
    "#print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "#for i in range(min(3, len(chunks))):\n",
    "#    print(f\"\\n--- Chunk {i+1} ---\\n{chunks[i]}\")\n",
    "\n",
    "output_path = os.path.join(output_chunks_dir, \"chunks_raw.json\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74184109",
   "metadata": {},
   "source": [
    "### Observations After Initial Chunking\n",
    "\n",
    "After extracting text from the PDF and splitting it into equal-sized chunks, we observed some issues that could negatively impact retrieval quality:\n",
    "\n",
    "1. **Unnecessary characters**:  \n",
    "   Some chunks contain page numbers, headers, footers, or stray symbols from the PDF. These elements do not carry semantic meaning and can confuse the retrieval model.\n",
    "\n",
    "2. **Breaking of text between chunks**:  \n",
    "   Since the chunks are created purely based on fixed size, sentences or paragraphs are often split in the middle. This can result in chunks that are **incomplete or hard to interpret**, reducing the effectiveness of semantic search.\n",
    "\n",
    "3. **Impact on retrieval**:  \n",
    "   Feeding these imperfect chunks into a RAG pipeline can lead to:\n",
    "   - Lower relevance of retrieved results\n",
    "   - Fragmented context\n",
    "   - Potential hallucinations in generated responses\n",
    "\n",
    "To improve retrieval quality, we need to **clean the text** and consider **smarter chunking strategies**, such as splitting by paragraphs or semantic boundaries.\n",
    "Below is the snapshot of chunk after initial fixed size chunking\n",
    "\n",
    "![Chunking Issue](https://github.com/Kunal627/rag-by-example/blob/main/data/images/raw_fixed_size.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5acbb496",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for chunk in chunks:\n",
    "    cleaned_chunk = clean_text(chunk[\"content\"])\n",
    "    cleaned_text.append({\"content\": cleaned_chunk})\n",
    "\n",
    "output_path = os.path.join(output_chunks_dir, \"chunks_cleaned.json\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_text, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6cac8",
   "metadata": {},
   "source": [
    "### Minimal Text Cleaning and Its Limitations\n",
    "\n",
    "In this notebook, I have applied **minimal cleaning techniques** to the PDF text:\n",
    "\n",
    "- Replacing newline characters (`\\n`) with spaces.  \n",
    "- Removing the word **\"Page\"** and numeric page numbers or ranges like `12-13`.  \n",
    "\n",
    "While these steps help **make the chunks cleaner and easier for a model to read**, there are some important caveats:\n",
    "\n",
    "1. **Context breaks:**  \n",
    "   Since chunks were created using **fixed-size splitting**, sentences or paragraphs may be split in the middle, which can lead to **loss of context**.\n",
    "\n",
    "2. **Text order issues:**  \n",
    "   PDFs often mix text with images, tables, or multi-column layouts. Minimal cleaning cannot always preserve the **logical flow of information**, so text might appear **out of order**.\n",
    "\n",
    "3. **Domain-specific cleaning:**  \n",
    "   More advanced cleaning, like handling footnotes, references, special symbols, or domain-specific abbreviations, depends on the **complexity of the PDF and the domain**. I leave this as an **exercise for the reader** to explore and implement.\n",
    "\n",
    "---\n",
    "\n",
    "**Example of including images after cleaning**:\n",
    "\n",
    "![Original pdf text](https://github.com/Kunal627/rag-by-example/blob/main/data/images/cleaned_fixed_size.PNG)\n",
    "![Chunking Issues with Text & Images](https://github.com/Kunal627/rag-by-example/blob/main/data/images/funfacts.PNG)\n",
    "\n",
    "By combining **minimal cleaning** with **semantic chunking**, we can produce chunks that are **both clean and contextually meaningful**, improving the performance of your retrieval-augmented generation pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
